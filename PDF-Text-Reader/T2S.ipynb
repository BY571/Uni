{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Text to Speech Reader\n",
    "\n",
    "- Python PDF Reader :\n",
    "- Textract / pip3 install textract\n",
    "- PyPDF2 / pip install PyPDF2\n",
    "___________________________________\n",
    "Python text to speech Librarys:\n",
    " https://pythonprogramminglanguage.com/text-to-speech/\n",
    " \n",
    "Google TTs engine : (import gtts)\n",
    "\n",
    "IBM tts watson: (import tts-watson)\n",
    " \n",
    " ______________________________\n",
    " Googletrans for txt übersetzung in dt/ andere sprache\n",
    " ____________________________\n",
    " \n",
    " Wenn text2speech (T2S) fertig ist mit Django testen pdf für ersten schritte bereits gedownloaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# creating an object \n",
    "file = open('Simultaneous Control and Human Feedback in the Training of a Robotic Agent with Actor-Critic Reinforcement Learning.pdf', 'rb')\n",
    "\n",
    "# creating a pdf reader object\n",
    "fileReader = PyPDF2.PdfFileReader(file)\n",
    "\n",
    "\n",
    "# print the number of pages in pdf file\n",
    "print(fileReader.numPages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract  This  paper  contributes  a preliminary report on the  advantages and disadvantages of incorporating  simultaneous human control and feedb ack signals  in the training of a reinforcement learning robotic  agent.  While  robotic  human -machi ne inte rfaces  have become increasingly  complex  in  both  form  and function , control  remains  challenging  for users . This  has resulted in  an increasing gap  between user  control approaches  and  the  number of  robotic  actu- ators  which can be controlled . One  way  to address  this gap is  to shift  autonomy to the  robot . Semi -autonomous actions of the  robotic  agent can  then  be shaped  and refined  by human  feedback , simpli- fying user  control . Most prior work  on human -agent shaping  has  incorporated  training with feed- back, or  has included  indirect control signals.  By  contrast,  in  this paper  we explore  how a human can  provide  concurrent  feedback  signals a nd real -time  myoelectric  control sig nals  to train a robotÕs  actor -critic reinforcement learning  control  system . Using  both a physical and a simulated robotic system, w e compare  training  performance  on a simple  move- ment  task  when  reward  is  derived  from the envi- ronment,  from the human, and from the combina- tion of the two . Our results indicate that  benefit  may  be gained  by including  human  generated feed- back  in learning algorithms for this complex hu- man -machine interactive domain . 1 Introduction  Human -robot  interaction is  becoming more complex with  the advancement  of  actuator and sensor  technologies.  One  key exa mple is that of robotic  prostheses : artificial limbs  attached to the body to replace abilities lost through injury  or illness.  Prosth etic  limbs  that  have  comparable  degrees of  freedom  (DoF)  and movement to human limbs  have now  been developed . A  principal  limitation  is  the  complex  con- trol of such devices  [Castellini  et al. , 2014]  by humans . One  of the  main reason s that users reject the use of prosthetic  devices  is the  fun ctional limitations of the limb. W hile state -of-the -art prosthetic limbs  can perform complex functions  and movements,  control of  this  functionality  by humans  is  still limited  [Biddiss  et al. , 2007].  New methods must be  developed to  help humans  control  complex robotic devices  that  are  directly connected to  them . Furthermore , s uch methods  should  incorporate some form of ongoing learning  so that the device can adapt to the human  who  is wearing it.   Myoelectric prosthese s are a class of modern robotic  prosthesis which monitor electrical signals produced by  muscle tissue in the patie ntÕs residual limb and use these  signals to control the movement of a multiple -actuator ro- botic appendage [Parker et al., 2006]. Myoelectric  control  aims to remove functio nality b arriers for patients, but  can be  a challenge for new amputees ; the transition  to a powered  prosthesis often requ ires  extensive training and  repeated  calibration  of the limb by clinicians . This difficulty is  partly  due to the  userÕs  inability to  provid e clear  electromyograph- ic (EMG)  control signals,  and  partly  due to  control  chal- len ges in  interpret ing  these  complex multi -dimensional  sig- nals  to guide  robotic movements  [Castellini  et al. , 2014] .  Another major limitation in most of the current prosthetic  control schemes is the lack of adaptation over time. The  need for adaptation may s tem from change s in the patient  physical and mental  state s, intents,  and /or  usage [Sensinger  et al.,  2009].  Changing  learned control  policies  requires  expert knowledge of  a patientÕs  physiology and prosthetic  hardware.  Most users can not adaptively  improve  the control  of their robotic  limb  independently , outside of the clinic.    A significant  amount of  past work has been done to ad- dress  the se challenge s by incorporating  machine  learning  into  prosthetic  control  systems  [Castellini  et al. , 2014, Par- ker  et al., 2006] . Examples include offline, supervised learn- ing methods of dimensionality reduction  [Englehart  et al.,  2003, Artemiadis  et al.,  2010], artificial neural network s,  and support vector machines [Oskoei and Hu,  2008], as well  as u nsupervised  [Sensinger  et al.,  2009], and semi -supervised [Nishikawa  et al.,  2001]  techniques. A recent  review by Castellini et al. provides  a good overview of the  state -of-the -art myoelectric prosthetic control research  [Cas- tellini  et al. , 2014 ].  Adaptive , real -time  approaches  to ad- dressing challenges in myoelectric control ha ve also  been  proposed and shown to work in simulation, and in prelimi- nary experiments  with able -bodied subjects and subjects  with amputations  [Edwards  et al ., 2015] . As well, previous  Simultaneous Control and Human Feedback in the Training   of a Robotic Agent with Actor -Critic Reinforcement Learning    Kory Mathewson  and  Patrick M. Pilarski  Dep artments of  Computing Science  and  Medicine  University  of Alberta , Edmonton, Alberta, Canada  [korym, pilarski] @ ualberta.ca   '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Text from Page\n",
    "pageObj = fileReader.getPage(0)\n",
    "# Rotate more infos: https://www.geeksforgeeks.org/working-with-pdf-files-in-python/\n",
    "# pageObj.rotateClockwise(rotation)  \n",
    "page = pageObj.extractText()\n",
    "page.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actu\n",
      "ators\n",
      "actuators\n",
      "test ators\n",
      "simpli\n",
      "fying\n",
      "simplifying\n",
      "test fying\n",
      "feed\n",
      "back,\n",
      "feedback,\n",
      "test back,\n",
      "move\n",
      "ment\n",
      "movement\n",
      "test ment\n",
      "envi\n",
      "ronment,\n",
      "environment,\n",
      "test ronment,\n",
      "combina\n",
      "tion\n",
      "combination\n",
      "test tion\n",
      "feed\n",
      "back\n",
      "feedback\n",
      "test back\n",
      "hu\n",
      "man\n",
      "human\n",
      "test man\n",
      "con\n",
      "trol\n",
      "control\n",
      "test trol\n",
      "ro\n",
      "botic\n",
      "robotic\n",
      "test botic\n",
      "electromyograph\n",
      "ic\n",
      "electromyographic\n",
      "test ic\n",
      "chal\n",
      "len\n",
      "challen\n",
      "test len\n",
      "sig\n",
      "nals\n",
      "signals\n",
      "test nals\n",
      "ad\n",
      "dress\n",
      "address\n",
      "test dress\n",
      "Par\n",
      "ker\n",
      "Parker\n",
      "test ker\n",
      "learn\n",
      "ing\n",
      "learning\n",
      "test ing\n",
      "[Cas\n",
      "tellini\n",
      "[Castellini\n",
      "test tellini\n",
      "ad\n",
      "dressing\n",
      "addressing\n",
      "test dressing\n",
      "prelimi\n",
      "nary\n",
      "preliminary\n",
      "test nary\n"
     ]
    }
   ],
   "source": [
    "# remove Word-linebreaks\n",
    "page_list = page.split()\n",
    "new_list = []\n",
    "for index in range(len(page_list)):\n",
    "    if page_list[index][-1] == \"-\":\n",
    "        #print(page_list[index])\n",
    "        word = page_list[index]\n",
    "        print(word[:-1])\n",
    "        print(page_list[index+1])\n",
    "        comb = word[:-1]+page_list[index+1]\n",
    "        print(comb)\n",
    "        new_list.append(comb)\n",
    "    if page_list[index-1][-1] == \"-\":\n",
    "        print(\"test\",page_list[index])\n",
    "        pass\n",
    "    \n",
    "    new_list.append(page_list[index])\n",
    "\n",
    "for index in range(len(new_list)):\n",
    "    try:\n",
    "        if new_list[index][-1] == \"-\":\n",
    "            del new_list[index]\n",
    "            del new_list[index + 1]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract This paper contributes a preliminary report on the advantages and disadvantages of incorporating simultaneous human control and feedb ack signals in the training of a reinforcement learning robotic agent. While robotic human -machi ne inte rfaces have become increasingly complex in both form and function , control remains challenging for users . This has resulted in an increasing gap between user control approaches and the number of robotic actuators ators can be controlled . One way to address this gap is to shift autonomy to the robot . Semi -autonomous actions of the robotic agent can then be shaped and refined by human feedback , simplifying fying control . Most prior work on human -agent shaping has incorporated training with feedback, back, has included indirect control signals. By contrast, in this paper we explore how a human can provide concurrent feedback signals a nd real -time myoelectric control sig nals to train a robotÕs actor -critic reinforcement learning control system . Using both a physical and a simulated robotic system, w e compare training performance on a simple movement ment when reward is derived from the environment, ronment, the human, and from the combination tion the two . Our results indicate that benefit may be gained by including human generated feedback back learning algorithms for this complex human man interactive domain . 1 Introduction Human -robot interaction is becoming more complex with the advancement of actuator and sensor technologies. One key exa mple is that of robotic prostheses : artificial limbs attached to the body to replace abilities lost through injury or illness. Prosth etic limbs that have comparable degrees of freedom (DoF) and movement to human limbs have now been developed . A principal limitation is the complex control trol such devices [Castellini et al. , 2014] by humans . One of the main reason s that users reject the use of prosthetic devices is the fun ctional limitations of the limb. W hile state -of-the -art prosthetic limbs can perform complex functions and movements, control of this functionality by humans is still limited [Biddiss et al. , 2007]. New methods must be developed to help humans control complex robotic devices that are directly connected to them . Furthermore , s uch methods should incorporate some form of ongoing learning so that the device can adapt to the human who is wearing it. Myoelectric prosthese s are a class of modern robotic prosthesis which monitor electrical signals produced by muscle tissue in the patie ntÕs residual limb and use these signals to control the movement of a multiple -actuator robotic botic [Parker et al., 2006]. Myoelectric control aims to remove functio nality b arriers for patients, but can be a challenge for new amputees ; the transition to a powered prosthesis often requ ires extensive training and repeated calibration of the limb by clinicians . This difficulty is partly due to the userÕs inability to provid e clear electromyographic ic control signals, and partly due to control challen len in interpret ing these complex multi -dimensional signals nals guide robotic movements [Castellini et al. , 2014] . Another major limitation in most of the current prosthetic control schemes is the lack of adaptation over time. The need for adaptation may s tem from change s in the patient physical and mental state s, intents, and /or usage [Sensinger et al., 2009]. Changing learned control policies requires expert knowledge of a patientÕs physiology and prosthetic hardware. Most users can not adaptively improve the control of their robotic limb independently , outside of the clinic. A significant amount of past work has been done to address dress se challenge s by incorporating machine learning into prosthetic control systems [Castellini et al. , 2014, Parker ker al., 2006] . Examples include offline, supervised learning ing of dimensionality reduction [Englehart et al., 2003, Artemiadis et al., 2010], artificial neural network s, and support vector machines [Oskoei and Hu, 2008], as well as u nsupervised [Sensinger et al., 2009], and semi -supervised [Nishikawa et al., 2001] techniques. A recent review by Castellini et al. provides a good overview of the state -of-the -art myoelectric prosthetic control research [Castellini tellini al. , 2014 ]. Adaptive , real -time approaches to addressing dressing in myoelectric control ha ve also been proposed and shown to work in simulation, and in preliminary nary with able -bodied subjects and subjects with amputations [Edwards et al ., 2015] . As well, previous Simultaneous Control and Human Feedback in the Training of a Robotic Agent with Actor -Critic Reinforcement Learning Kory Mathewson and Patrick M. Pilarski Dep artments of Computing Science and Medicine University of Alberta , Edmonton, Alberta, Canada [korym, pilarski] @ ualberta.ca'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "tts = gTTS(text= page, lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.save(\"test.mp3\")\n",
    "os.system(\"mpg321 test.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_list = []\n",
    "for i in new_list:\n",
    "    try:\n",
    "        #print(i)\n",
    "        übersetzung = translator.translate(i,dest = \"de\")\n",
    "        #print(übersetzung.text)\n",
    "        trans_list.append(übersetzung.text)\n",
    "    except:\n",
    "        print(\"Error_word: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstrakt Dies Papier- trägt dazu bei ein vorläufig Bericht auf das Vorteile und Nachteile von Einarbeitung gleichzeitig Mensch Steuerung und Feedb oh Signale in das Ausbildung von ein Verstärkung Lernen Roboter Agent. Während Roboter Mensch Eine Stadt nicht nicht Rfaces haben werden zunehmend Komplex in beide form und Funktion , Steuerung Überreste herausfordernd zum Benutzer . Dies hat resultierte in an steigend Spalt zwischen Benutzer Steuerung Ansätze und das Nummer von Roboter Aktuatoren Atoren kann Sein kontrolliert . Ein Weg zu Adresse Dies Spalt ist zu Verschiebung Autonomie zu das Roboter . Halb -autonom Aktionen von das Roboter Agent kann dann Sein geformt und raffiniert durch Mensch feedback , vereinfacht feuernd Steuerung . Die meisten vorher Arbeit auf Mensch -Agent Gestaltung hat eingearbeitet Ausbildung mit feedback, zurück, hat inbegriffen indirekt Steuerung Signale. Durch Kontrast, in Dies Papier- wir erkunden Wie ein Mensch kann zu Verfügung stellen gleichzeitig feedback Signale ein nd echt -Zeit myoelektrisch Steuerung zu nals zu Zug ein Roboter Darsteller -Kritiker Verstärkung Lernen Steuerung System . Verwenden beide ein physisch und ein simuliert Roboter System, in und vergleichen Ausbildung Performance auf ein einfach Bewegung ment wann Belohnung ist abgeleitet von das Umgebung, ronment, das Mensch, und von das Kombination tion das zwei . Unser Ergebnisse zeigen Das Vorteil kann Sein gewonnen durch einschließlich Mensch erzeugt feedback zurück Lernen Algorithmen zum Dies Komplex Mensch man interaktiv domain . 1 Einführung Mensch -Roboter Interaktion ist Werden Mehr Komplex mit das Förderung von Aktuator und Sensor Technologien. Ein Schlüssel Exa mple ist Das von Roboter Prothesen : künstlich Gliedmaßen angebracht zu das Karosserie zu ersetzen Fähigkeiten hat verloren durch Verletzung oder Krankheit. Prost ethisch Gliedmaßen Das haben vergleichbar Grad von Freiheit (DoF) und Bewegung zu Mensch Gliedmaßen haben jetzt gewesen entwickelt . EIN Schulleiter Einschränkung ist das Komplex Steuerung steuern eine solche Geräte [Castellini und al , 2014] durch Menschen . Ein von das Main Grund s Das Benutzer ablehnen das benutzen von prothetisch Geräte ist das Spaß optional Einschränkungen von das Glied. die Betrug Zustand -des -art prothetisch Gliedmaßen kann ausführen Komplex Funktionen und Bewegungen, Steuerung von Dies Funktionalität durch Menschen ist immer noch begrenzt [Biddiss und al , 2007]. Neu Methoden Muss Sein entwickelt zu Hilfe Menschen Steuerung Komplex Roboter Geräte Das sind direkt in Verbindung gebracht zu Sie . Außerdem , s uch Methoden sollte Übernehmen etwas form von laufend Lernen so Das das Gerät kann anpassen zu das Mensch Wer ist Tragen es. Myoelektrische er fügte hinzu s sind ein Klasse von modern Roboter Prothese welche Monitor elektrisch Signale produziert durch Muskel Gewebe in das patie zB Restwert Glied und benutzen diese Signale zu Steuerung das Bewegung von ein mehrere -aktuator Roboter botisch [Parker und al., 2006]. Myoelektrische Steuerung Ziele zu Löschen Eine Funktion Nähe b arriers zum Patienten, aber kann Sein ein Herausforderung zum Neu Amputierte ; das Übergang zu ein angetrieben Prothese häufig Anfrage IRES umfangreich Ausbildung und wiederholt Kalibrierung von das Glied durch Kliniker . Dies Schwierigkeit ist teilweise fällig zu das Benutzer Unfähigkeit zu bereit und klar Elektromyographisch ic Steuerung Signale, und teilweise fällig zu Steuerung herausfordern nur in interpretieren Ing diese Komplex multi -dimensional Signale nals führen Roboter Bewegungen [Castellini und al , 2014] . Ein weiterer Haupt Einschränkung in die meisten von das Strom prothetisch Steuerung Schemata ist das Mangel von Anpassung Über Zeit. Das brauchen zum Anpassung kann s hat von Veränderung s in das geduldig physisch und geistig Zustand s, Absichten, und /oder Verwendung Sensinger und al., 2009]. Ändern gelernt Steuerung Richtlinien erfordert Experte Wissen von ein patientÕs Physiologie und prothetisch hardware. Die meisten Benutzer kann nicht adaptiv verbessern das Steuerung von ihr Roboter Glied unabhängig , draußen von das Klinik. EIN von Bedeutung Menge von Vergangenheit Arbeit hat gewesen erledigt zu Adresse Kleid es Herausforderung s durch Einarbeitung Maschine Lernen in prothetisch Steuerung Systeme [Castellini und al , 2014, Parker weil al., 2006] . Beispiele einschließen offline, überwacht Lernen Ing von Dimensionalität die Ermäßigung [Englehart und al., 2003, Artemiadis und al., 2010], künstlich neural Netzwerk s, und Unterstützung Vektor Maschinen [Oskoei und Ist, 2008], wie Gut wie u unbeaufsichtigt Sensinger und al., 2009], und halb -überwacht \"Nishikawa und al., 2001] Techniken. EIN kürzlich Rezension durch Castellini und al bietet ein gut Überblick von das Zustand -des -art myoelektrisch prothetisch Steuerung Forschung [Castellini Tellini al , 2014 ]. Anpassungsfähig , echt -Zeit Ansätze zu Adressierung Dressing in myoelektrisch Steuerung hat und ebenfalls gewesen vorgeschlagen und gezeigt zu Arbeit in Simulation, und in vorläufig nicht mit fähig Körper Fächer und Fächer mit Amputationen [Edwards und zu ., 2015] . Wie Gut, Bisherige Gleichzeitig Steuerung und Mensch Feedback in das Ausbildung von ein Roboter Agent mit Darsteller -Kritiker Verstärkung Lernen Kory Mathewson und Patrick M. Pilarski Dep Wohnungen von Rechnen Wissenschaft und Medizin Universität von Alberta , Edmonton, Alberta, Kanada [Kórym. Pilarski] @ ualberta.ca'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(trans_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
